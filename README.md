Simple Data Science Project
=====================================

一个简单的数据科学项目，通过一系列步骤，实现了从数据采集，数据可视化到数据建模与预测这一完整的数据分析流程。

整个项目分为以下部分：

1. 新闻数据采集:    
使用scrapy爬取用于训练的数据。    
这里写了两类爬虫:    
    1) 搜狐新闻：爬取数量相对较多。静态网页较为容易获取。    
    2) 腾讯新闻：涉及动态网页的爬取，借助phantomjs这一工具，通过爬虫模拟前端操作以获取数据。    
这里分别爬取了两个网站的新闻，考虑时间跨度等多方面因素，选择了搜狐新闻作为数据集。    
		
2. 词频统计与词云图绘制:    
统计不同类别的新闻的高频词，并以词云的形式展示。    
词云图是一种直观有效的可视化工具，能够更加有效的表现文字重点信息。    
为了保证其效果，采用去除停用词，词云平滑等方案。    

3. 统计主题与数据可视化:    
分析每类新闻中，大家最关心的主题并加以可视化。    
步骤包括：    
   读取数据 --> 分析数据 --> 作图    
这里分析数据有一个考量:    
   搜狐新闻的评论数和浏览数较少，大多为0，没有采集的意义，不足以当做权重。    
   新闻的内容，如上节所述，包含大量的常用词，统计词频，价值不大。    
   新闻的标题，包含大量的主题词，可以用来作为关键词，但是仍然存在常用词与停用词，影响最终效果。    
   这里的一个处理方法是：使用停用词表做过滤，然后手动补充一些常用词，随着经常的迭代可以做到提取主题词。    
   而且包含标题的情况下，没有必要使用模型再次提取主题。    
   同时对新闻出版社也做基本的展示。    

4. 建立模型预测新闻类别:    
建立模型，对新闻文本进行分类(包括特征抽取，分类器选择，分类器参数调优，结果评估等过程)。这里的顺序同上。    
整个机器学习流程为：    
   特征抽取：    
       选择有价值的数据内容做特征提取。对文本分类选择内容作为输入，分别按照词频和逆文本频率进行提取。按照if-idf可以有效提取关键词，而不是使与主题无关的常用词有较高的权重。    
   分类器选择：    
       由于数据维度高且稀疏，选择逻辑斯蒂回归模型、朴素贝叶斯模型和K最近邻模型(K近邻可以去掉)模型作为候选模型。    
   分类器参数调优：    
       使用GridSearchCV进行检查验证与参数调优。    
   结果评估：    
       例如直接度量其准确度，或者使用混淆矩阵。其中混淆矩阵反映了分类的细节，例如A被分为A、B、C的数量。    
在本例中，使用TF-IDF词向量做词特征，使用朴素贝叶斯模型训练预测，效果好且速度快，适合用于这一场景。    
